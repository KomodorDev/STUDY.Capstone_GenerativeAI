{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec5c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdev.com/building-visionrag-ai-powered-image-search-with-llama-3-2-qdrant-and-litserve-10bf22df5d41\n",
    "#https://medium.com/kx-systems/guide-to-multimodal-rag-for-images-and-text-10dab36e3117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1060ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c0746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxx\" # Replace with your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24ce281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 1. GPT-4o OCR =========\n",
    "def gpt4o_ocr(image_path, api_key=None):\n",
    "    # Always get the API key from argument or environment\n",
    "    api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "    endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer  {api_key}\"}\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",   # or \"gpt-4o\"\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Tell me the brand of this battery. Just give me the brand name. Do not give other explanation and text. If you cannot recogize, Just tell us NA:\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img_b64}\"}},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "    }\n",
    "    resp = requests.post(endpoint, headers=headers, json=payload)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b32c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 2. Embedding =========\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Text embedding (GTE-base)\n",
    "text_model = SentenceTransformer(\"thenlper/gte-base\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5225fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image embedding (CLIP)\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9c0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    return text_model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "def embed_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = clip_model.get_image_features(**inputs)\n",
    "    return emb.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f1b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 3. Qdrant =========\n",
    "client = QdrantClient(path=\"/qdrant_data\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "702c9f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install collection, including text_embedding + image_embedding\n",
    "client.recreate_collection(\n",
    "    collection_name=\"multimodal_docs\",\n",
    "    vectors_config={\n",
    "        \"text_embedding\": VectorParams(size=768, distance=Distance.COSINE),\n",
    "        \"image_embedding\": VectorParams(size=512, distance=Distance.COSINE),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cec329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 4. Put images into ocr, text embedding, image embedding =========\n",
    "def insert_image_doc(image_path):\n",
    "    ocr_text = gpt4o_ocr(image_path)\n",
    "    text_vec = embed_text(ocr_text)\n",
    "    img_vec = embed_image(image_path)\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    client.upsert(\n",
    "        collection_name=\"multimodal_docs\",\n",
    "        points=[\n",
    "            PointStruct(\n",
    "                id=point_id,\n",
    "                vector={\n",
    "                    \"text_embedding\": text_vec.tolist(),\n",
    "                    \"image_embedding\": img_vec.tolist(),\n",
    "                },\n",
    "                payload={\n",
    "                    \"source\": image_path,\n",
    "                    \"ocr_text\": ocr_text,\n",
    "                },\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    print(f\"Inserted {image_path} with OCR: {ocr_text[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6dc79b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 5. Query =========\n",
    "def search_by_text(query, top_k=3):\n",
    "    q_vec = embed_text(query)\n",
    "    results = client.search(\n",
    "        collection_name=\"multimodal_docs\",\n",
    "        query_vector=(\"text_embedding\", q_vec.tolist()),\n",
    "        limit=top_k,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def search_by_image(image_path, top_k=3):\n",
    "    q_vec = embed_image(image_path)\n",
    "    results = client.search(\n",
    "        collection_name=\"multimodal_docs\",\n",
    "        query_vector=(\"image_embedding\", q_vec.tolist()),\n",
    "        limit=top_k,\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93169e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# ========= 1. Read all images in the folder =========\n",
    "def insert_folder_images(folder_path):\n",
    "    image_paths = glob(os.path.join(folder_path, \"*.png\")) + \\\n",
    "                  glob(os.path.join(folder_path, \"*.jpg\")) + \\\n",
    "                  glob(os.path.join(folder_path, \"*.jpeg\"))\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            insert_image_doc(img_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed {img_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "613aa202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 2. Query function: text, image, or hybrid =========\n",
    "def rag_query(query_text=None, query_image=None, mode=\"text\", top_k=3):\n",
    "    if mode == \"text\":\n",
    "        return search_by_text(query_text, top_k)\n",
    "    elif mode == \"image\":\n",
    "        return search_by_image(query_image, top_k)\n",
    "    elif mode == \"hybrid\":\n",
    "        return search_hybrid(query_text=query_text, query_image=query_image, alpha=0.7, top_k=top_k)\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'text', 'image', or 'hybrid'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d0c4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder = \"image/rag\"  # Image folder path\n",
    "    insert_folder_images(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95a7e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Text Query RAG:\")\n",
    "hits = rag_query(query_text=\"Give me panasonic battery images\", mode=\"text\", top_k=3)\n",
    "for h in hits:\n",
    "    print(h.payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2959f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüñº Image Query RAG:\")\n",
    "hits = rag_query(query_image=\"image/Panasonic-Alkaline-AA.jpg\", mode=\"image\", top_k=3)\n",
    "\n",
    "for h in hits:\n",
    "    print(h.payload)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
